a. The first algorithm is pure decision tree, so the accuracy is always 100%. The second algorithm is decision tree as well, but if the current node has information entropy lower than 0.55, we stop the recursion and return the mode value of current node. <br/>
b. The 1st dataset is a small dataset. There is no difference between algorithm 1 and algorithm 2. The 2nd dataset is larger with 8 attributes and 150 rows. We can see that algorithm 1 needs 61 if-else clauses to reach accuracy 100%. However, algorithm 2 only need 28 if-else clauses and reaches accuracy 85%. Algorithm 2 has better efficiency. <br/>
c. From random dataset, we can see the trend. The more rows in a dataset, the more difference in if-else counts between algorithm 2 and algorithm 1. For dataset with few rows, there is not much difference. For algorithm 1, mostly it needs half of numbers of rows if-else clauses. Besides, the accuracy of algorithm 2 is still high (> 80%). Algorithm 2 does some compression to generalize the pattern while keep the high accuracy.